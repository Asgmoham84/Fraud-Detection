{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"# NLP with Job Posting Data <br><br>\n\nIn this study we are going to deveop a machine learning model to predict the skills that are important for each job. We will use a data set of job postings data from indeed.ca\n\nThis data set has the job postings information related to 4 job titles: Data Scientist, Carpenter, Registered Nurse, and Customer Service. Using this data set, we are going to predict the top 10 most important skills for each of these titles."},{"metadata":{},"cell_type":"markdown","source":"https://monkeylearn.com/keyword-extraction/\n\nWhat is Keyword Extraction?\n\nKeyword extraction (also known as keyword detection or keyword analysis) is a text analysis technique that consists of automatically extracting the most important words and expressions in a text. It helps summarize the content of a text and recognize the main topics which are being discussed. "},{"metadata":{},"cell_type":"markdown","source":"## Importing Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"\ndf = pd.read_csv(\"../input/jobs-ds-carp/Jobs.csv\", index_col=False)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Cleaning"},{"metadata":{},"cell_type":"markdown","source":"According to IBM Data Analytics you can expect to spend up to 80% of your time cleaning data. \nhttps://towardsdatascience.com/data-cleaning-with-python-and-pandas-detecting-missing-values-3e9c6ebcf78b"},{"metadata":{},"cell_type":"markdown","source":"![](http://)First we need to take care of the missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.keys()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removing the extra columns\n\ndf=df[['Job Title', 'Company Name', 'Location',\n       'job URL', 'Job Description', 'Group']]\ndf.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Validating if there is any null value in each column, if the result is False, it means there is no null value in that column\n\nfor column in df.columns:\n    if (df['Job Title'].isnull().unique() ==True):\n        print ('column', column, 'has missing values')\n    else:\n        print ('column', column, 'is valid and has no missing values')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Getting the number of job postings by job title groups"},{"metadata":{"trusted":true},"cell_type":"code","source":"Number_of_Jobs = df.groupby('Group')['Job Title'].count()\nNumber_of_Jobs.columns=['Job Title ID', 'Number of Job Postings']\nNumber_of_Jobs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#xvals = Number_of_Jobs['Job Title ID']\n#yvals = Number_of_Jobs['Number of Job Postings']\n\n# Generating a bar chart for number of job postings by city:\n\n# Getting number of job postings by city:\n#jobs_by_city= df.groupby('Location')['Job Title'].count().sort_values(ascending=False)[:10]\n#print('The number of prescriptions by the antibiotic form:\\n', jobs_by_city)\ncolors = ['green', '#006fb9','#006fb9','#006fb9']\n\n\nplt.figure(figsize=(10,8))\nxvals = Number_of_Jobs.index\nyvals = Number_of_Jobs.tolist()\nprint(xvals)\nprint(yvals)\nplt.bar(xvals, yvals, color=colors)\nplt.xticks(rotation=90)\n#plt.margins(0.2)\n\nplt.subplots_adjust(bottom=0.3, left=0.2)\nplt.title('Number of Data Science Jobs by City (Top 10 Cities)')\nplt.xlabel('City')\nplt.ylabel('Number of Job Postings')\nplt.xticks(np.arange(min(xvals), max(xvals)+1, 1.0),('Data Scientist','Carpenter','Registered Nurse','Customer Service Rep.'))\n#plt.xticks('A','B','C','D')\n\n#Having the y axis formatted as thousand separated\nax = plt.gca()\nax.get_yaxis().set_major_formatter(plt.FuncFormatter(lambda x, loc: \"{:,}\".format(int(x))))\n\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\n\nfor i, v in enumerate(yvals):\n    ax.text(i+0.9, v+10, str(v))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## First we will take only two job titles, Data Scientist and Carpenter"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df[df['Group']<3]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Group'] = np.where(df['Group']==1,0,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Number_of_Jobs = df.groupby('Group')['Job Title'].count()\nNumber_of_Jobs.columns=['Job Title ID', 'Number of Job Postings']\n\ncolors = ['green', '#006fb9','#006fb9','#006fb9']\n\n\nplt.figure(figsize=(7,9))\nxvals = Number_of_Jobs.index\nyvals = Number_of_Jobs.tolist()\nprint(xvals)\nprint(yvals)\nplt.bar(xvals, yvals, color=colors)\nplt.xticks(rotation=90)\n#plt.margins(0.2)\n\nplt.subplots_adjust(bottom=0.3, left=0.2)\nplt.title('Number of Data Science Jobs by City (Top 10 Cities)')\nplt.xlabel('City')\nplt.ylabel('Number of Job Postings')\nplt.xticks(np.arange(min(xvals), max(xvals)+1, 1.0),('Data Scientist','Carpenter','Registered Nurse','Customer Service Rep.'))\n#plt.xticks('A','B','C','D')\n\n#Having the y axis formatted as thousand separated\nax = plt.gca()\nax.get_yaxis().set_major_formatter(plt.FuncFormatter(lambda x, loc: \"{:,}\".format(int(x))))\n\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\n\nfor i, v in enumerate(yvals):\n    ax.text(i-0.05, v+10, str(v))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## As you see, the data set is unbalanced"},{"metadata":{},"cell_type":"markdown","source":"# Machine Learning with Sklearn"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Group.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Split data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(df['Job Description'],\n                                                   df['Group'],\n                                                   random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('X_train first entry:\\n\\n', X_train.iloc[0])\nprint('\\n\\nX_train shape: ', X_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## CountVectorizer"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\n# Fit the CountVectorizer to the training data\nvect = CountVectorizer().fit(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vect.get_feature_names()[::1000]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Finding the number of features after the CountVectorizer"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('We now have',len(vect.get_feature_names()),'features after fitting the CountVectorizer on training set')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# transform the documents in the training data to a document-term matrix\nX_train_vectorized = vect.transform(X_train)\n\nX_train_vectorized","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\n# Train the model\nmodel = LogisticRegression().fit(X_train_vectorized, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\n\n# Predict the transformed test documents\nprediction = model.predict(vect.transform(X_test))\n\nprint('AUC:', roc_auc_score(y_test,prediction))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get feature names as numpy array\nfeature_names = np.array(vect.get_feature_names())\n\n# Sort the coefficients from the model\nsorted_coef_index = model.coef_[0].argsort()\n#print(sorted_coef_index)\n# Find the 10 smalles and 10 largest coefficients\n# The 10 largest coefficients are bein indexed using [:-11:-1]\n\nprint('Coefficients related to job title 1 (Data Scientist):\\n {}\\n'.format(feature_names[sorted_coef_index[:20]]))\nprint('Coefficients related to job title 2 (Carpenter):\\n {}\\n'.format(feature_names[sorted_coef_index[:-21:-1]]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Improving the results:\n\nAs you can see, the model was able to find the keywords related to each job title. But there are still some words that are not related, such as \"with\", \"unknown\", and \"00\".<br>\nFor improving the results, we are going to use Tfidf\n\nAlso you can see that the model was able to find the keywords \"machine\", and \"learning\" related to Data Scientist job title separately, but the model was not able to detect that Machine Learning is considered as one term. We will use n-grams to improve the model regarding this issue."},{"metadata":{},"cell_type":"markdown","source":"# Tfidf"},{"metadata":{},"cell_type":"markdown","source":"1. Tfidf is an statistical approach<br><br>\n\n**Tf–idf, or Term frequency-inverse document frequency, allows us to weight terms based on how important they are to a document.\nHigh weight is given to terms that appear often in a particular document, but don't appear often in the corpus.\nFeatures with low tf–idf are either commonly used across all documents or rarely used and only occur in long documents.\nFeatures with high tf–idf are frequently used within specific documents, but rarely used across all documents. "},{"metadata":{},"cell_type":"markdown","source":"## There is a possibility that Tfidf cant be a good aproach, because this metric calculates the number of times a word appears in a text (term frequency) and compares it with the inverse document frequency (how rare or common that word is in the entire data set)[](http://)\n\nAlso the words that appear more frequently in a group of documents are not necessarily the most relevant. Likewise, a word that appears in a single text but doesn’t appear in the remaining documents may be very important to understand the content of that text. "},{"metadata":{},"cell_type":"markdown","source":"# Now we are going to use RAKE (Rapid Automated Heyword Extraction)\n\n"},{"metadata":{},"cell_type":"markdown","source":"# Also if we find a way to consider names only, that might be helpful as well. (Linguistic Approach)\nMost systems that use some kind of linguistic information outperform those that don’t do so. We strongly recommend that you try some of them when extracting keywords from your texts."},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\n# Fit the TfidfVectorizer (term frequency–inverse document frequency) to the training data specifiying a minimum document frequency of 5\nvect = TfidfVectorizer(min_df=50).fit(X_train)\nlen(vect.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here you can see that by using the Tfidf Vectorizer, "},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_vectorized = vect.transform(X_train)\n\nmodel = LogisticRegression()\nmodel.fit(X_train_vectorized, y_train)\n\npredictions = model.predict(vect.transform(X_test))\n\nprint('AUC: ', roc_auc_score(y_test, predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You can see that we could get the same AUC with about 1/4 of the features. "},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_names = np.array(vect.get_feature_names())\n\nsorted_tfidf_index = X_train_vectorized.max(0).toarray()[0].argsort()\n\n\nprint('Smallest tfidf:\\n{}\\n'.format(feature_names[sorted_tfidf_index[:10]]))\nprint('Largest tfidf: \\n{}'.format(feature_names[sorted_tfidf_index[:-11:-1]]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sorted_coef_index = model.coef_[0].argsort()\n\nprint('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:40]]))\nprint('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-41:-1]]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sorted_coef_index = model.coef_[0].argsort()\n\nprint('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:20]]))\nprint('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-21:-1]]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# n-grams <br><br>\n\nhttps://monkeylearn.com/keyword-extraction/\nWord Collocations and Co-occurrences\n\nAlso known as N-gram statistics, word collocations and co-occurrences can help you understand the semantic structure of a text and count separate words as one.\n\nCollocations are words that frequently go together. The most common types of collocations are bi-grams (two terms that appear adjacently, like ‘customer service’, ‘video calls’ or ‘email notification’) and tri-grams (a group of three words, like ‘easy to use’ or ‘social media channels’). "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Split data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(df['Job Description'],\n                                                   df['Group'],\n                                                   random_state=0)\n\n\n# Fit the CountVectorizer to the training data specifiying a minimum \n# document frequency of 5 and extracting 1-grams and 2-grams\nvect = CountVectorizer(min_df=100, ngram_range=(1,3)).fit(X_train)\n\nX_train_vectorized = vect.transform(X_train)\n\nlen(vect.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = LogisticRegression()\nmodel.fit(X_train_vectorized, y_train)\n\npredictions = model.predict(vect.transform(X_test))\n\nprint('AUC: ', roc_auc_score(y_test, predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_names = np.array(vect.get_feature_names())\n\nsorted_coef_index = model.coef_[0].argsort()\n\nprint('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:40]]))\nprint('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-41:-1]]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## It seems that the feature names are not sorted properly. Also we need to take care of stop words."},{"metadata":{},"cell_type":"markdown","source":"# Fixing the issue"},{"metadata":{},"cell_type":"markdown","source":"## Stopwords Removal[](http://)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# To see a list of stopwords in English\n\nimport nltk\nfrom nltk.corpus import stopwords\nset(stopwords.words('english'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Lowercasing the text"},{"metadata":{"trusted":true},"cell_type":"code","source":"#df['Job Description'] = df['Job Description'].apply([lambda text: stop_word_remover(text) ])\ndf['Job Description'] = df['Job Description'].str.lower().str.replace(',', ' ').str.replace('.' , ' ').str.replace('  ',' ')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Job Description'].iloc[500]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Stopword Removal using NLTK"},{"metadata":{"trusted":true},"cell_type":"code","source":"# The following code is to remove stop words from sentence using nltk\n# Created by - ANALYTICS VIDHYA\n\n# importing libraries\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize \nset(stopwords.words('english'))\n\ndef stop_word_remover(text):\n\n\n    # set of stop words\n    stop_words = set(stopwords.words('english')) \n    #print(stop_words)\n    # tokens of words  \n    word_tokens = word_tokenize(text) \n\n    filtered_sentence = [] \n\n    for w in word_tokens: \n        if w not in stop_words: \n            filtered_sentence.append(w) \n\n\n\n    #print(\"\\n\\nOriginal Sentence \\n\\n\")\n    #print(\" \".join(word_tokens)) \n    #print('***************************************')\n    \n    result = \" \".join(filtered_sentence)\n    #print(result) \n    return result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Job Description'] = df['Job Description'].apply(stop_word_remover)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Job Description'].iloc[500]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Another apreach: adding our set of words to the stop words<br><br>\n['i','me','my','myself','we','our','ours','ourselves','you','your','yours','yourself','yourselves','he','him','his','himself','she','her','hers','herself','it','its','itself','they','them','their','theirs','themselves','what','which','who','whom','this','that','these','those','am','is','are','was','were','be','been','being','have','has','had','having','do','does','did','doing','a','an','the','and','but','if','or','because','as','until','while','of','at','by','for','with','about','against','between','into','through','during','before','after','above','below','to','from','up','down','in','out','on','off','over','under','again','further','then','once','here','there','when','where','why','how','all','any','both','each','few','more','most','other','some','such','no','nor','not','only','own','same','so','than','too','very','s','t','can','will','just','don','should','now']\n\n\n# We should also do a stemming so that words like ‘search’ or ‘searched’ or ‘searching’ which all mean ‘search’. This process of reducing word to its root is called stemming\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#df.to_csv('test02.csv')\ndf['Group'].mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Split data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(df['Job Description'],\n                                                   df['Group'],\n                                                   random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\n# Fit the CountVectorizer to the training data\nvect = CountVectorizer().fit(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vect.get_feature_names()[::1000]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('We now have',len(vect.get_feature_names()),'features after fitting the CountVectorizer on training set')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# We are facing a wide data set (p>n issue) so we may need to add more samples to the data set"},{"metadata":{"trusted":true},"cell_type":"code","source":"# transform the documents in the training data to a document-term matrix\nX_train_vectorized = vect.transform(X_train)\n\nX_train_vectorized","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\n# Train the model\nmodel = LogisticRegression().fit(X_train_vectorized, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\n\n# Predict the transformed test documents\nprediction = model.predict(vect.transform(X_test))\n\nprint('AUC:', roc_auc_score(y_test,prediction))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get feature names as numpy array\nfeature_names = np.array(vect.get_feature_names())\n\n# Sort the coefficients from the model\nsorted_coef_index = model.coef_[0].argsort()\n#print(sorted_coef_index)\n# Find the 10 smalles and 10 largest coefficients\n# The 10 largest coefficients are bein indexed using [:-11:-1]\n\nprint('Coefficients related to job title 1 (Data Scientist):\\n {}\\n'.format(feature_names[sorted_coef_index[:20]]))\nprint('Coefficients related to job title 2 (Carpenter):\\n {}\\n'.format(feature_names[sorted_coef_index[:-21:-1]]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Tfidf"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\n# Fit the TfidfVectorizer (term frequency–inverse document frequency) to the training data specifiying a minimum document frequency of 5\nvect = TfidfVectorizer(min_df=100, ngram_range=(1,3)).fit(X_train)\nlen(vect.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_vectorized = vect.transform(X_train)\n\nmodel = LogisticRegression()\nmodel.fit(X_train_vectorized, y_train)\n\npredictions = model.predict(vect.transform(X_test))\n\nprint('AUC: ', roc_auc_score(y_test, predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_names = np.array(vect.get_feature_names())\n\nsorted_tfidf_index = X_train_vectorized.max(0).toarray()[0].argsort()\n\n\nprint('Smallest tfidf:\\n{}\\n'.format(feature_names[sorted_tfidf_index[:10]]))\nprint('Largest tfidf: \\n{}'.format(feature_names[sorted_tfidf_index[:-11:-1]]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sorted_coef_index = model.coef_[0].argsort()\n\nprint('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:40]]))\nprint('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-41:-1]]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Testing"},{"metadata":{"trusted":true},"cell_type":"code","source":"text = df['Job Description'].iloc[100]\ntext","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.predict(vect.transform(['we need someone to work with analysis']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input1 = \"Statistics statistical lists listing listings\"\nwords1 = input1.lower().split(' ')\nwords1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem.snowball import SnowballStemmer\nstemmer = SnowballStemmer(\"english\")\nprint(stemmer.stem(\"sql\"))\n\nprint(stemmer.stem(\"statistical\"))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nporter = nltk.PorterStemmer()\n[porter.stem(t) for t in words1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"import pandas as pd\nJobs = pd.read_csv(\"../input/jobs-ds-carp/Jobs.csv\")","execution_count":0,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"import pandas as pd\nJobs = pd.read_csv(\"../input/jobs-ds-carp/Jobs.csv\")","execution_count":0,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}